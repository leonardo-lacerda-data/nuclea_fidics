import feedparser
import os
import random
import time
import dateparser
from datetime import datetime, timedelta
from duckduckgo_search import DDGS
from pysentimiento import create_analyzer
from src.db_connection import get_connection

# CONFIGURA√á√ïES
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# RSS Links
TOPICOS_RSS = {
    'AGRO': 'https://news.google.com/rss/search?q=agroneg%C3%B3cio+brasil+safra&hl=pt-BR&gl=BR&ceid=BR:pt-419',
    'INDUSTRIA': 'https://news.google.com/rss/search?q=ind%C3%BAstria+brasil+desempenho&hl=pt-BR&gl=BR&ceid=BR:pt-419',
    'VAREJO': 'https://news.google.com/rss/search?q=varejo+vendas+brasil+economia&hl=pt-BR&gl=BR&ceid=BR:pt-419',
    'SERVICOS': 'https://news.google.com/rss/search?q=setor+servi%C3%A7os+crescimento+brasil&hl=pt-BR&gl=BR&ceid=BR:pt-419',
    'MERCADO': 'https://news.google.com/rss/search?q=mercado+financeiro+ibovespa+dolar&hl=pt-BR&gl=BR&ceid=BR:pt-419'
}

# TERMOS
TERMOS_HISTORICO = {
    'AGRO': [
        'Safra de soja Brasil resultados',
        'Agroneg√≥cio exporta√ß√£o desempenho',
        'Cr√©dito rural Plano Safra',
        'Pre√ßo commodities agr√≠colas hoje',
        'Colheita Brasil'
    ],
    'INDUSTRIA': [
        'Produ√ß√£o industrial IBGE desempenho',
        'Ind√∫stria automobil√≠stica Brasil',
        'Investimentos ind√∫stria nacional',
        'Sondagem industrial CNI'
    ],
    'VAREJO': [
        'Vendas varejo Brasil desempenho',
        'Balan√ßo e-commerce Brasil',
        'Expans√£o atacarejo Brasil',
        '√çndice de consumo das fam√≠lias'
    ],
    'SERVICOS': [
        'Volume de servi√ßos PMS IBGE',
        'Turismo Brasil faturamento',
        'Setor de log√≠stica e transportes',
        'Mercado de trabalho servi√ßos'
    ],
    'MERCADO': [
        'Ibovespa fechamento',
        'Relat√≥rio Focus Banco Central',
        'Balan√ßa comercial Brasil',
        'Resultado PIB trimestral'
    ]
}

# FONTES QUE ACEITAMOS
FONTES_ACEITAS = [
    'uol', 'globo', 'cnn', 'estadao', 'folha', 'veja', 'bbc', 'terra', 'r7',
    'metropoles', 'band', 'correio', 'infomoney', 'money', 'exame', 'valor',
    'forbes', 'sun', 'investing', 'agrolink', 'canal rural', 'cni', 'fdr',
    'monitor', 'seudinheiro', 'poder360', 'jota', 'migalhas', 'conjur',
    'valor economico', 'g1', 'bloomberg', 'reuters', 'ibge', 'ipea', 'cnn brasil'
]


def validar_fonte_por_texto(texto):
    texto = str(texto).lower()
    for fonte in FONTES_ACEITAS:
        if fonte in texto:
            return True
    return False


def analisar_sentimento(texto, analyzer):
    if not texto or len(texto) < 5: return 0.0
    try:
        resultado = analyzer.predict(texto)
        probs = resultado.probas
        return probs.get('POS', 0) - probs.get('NEG', 0)
    except:
        return 0.0


def validar_recencia(data_pub, dias_max=730):
    if not data_pub: return False
    data_corte = datetime.now().date() - timedelta(days=dias_max)
    if data_pub < data_corte:
        return False
    return True


def limpar_data_ddg(data_raw):
    """Trata datas do DuckDuckGo que v√™m em formatos variados."""
    if not data_raw: return None
    try:
        # DDG √†s vezes manda timestamp ISO, √†s vezes texto relativo
        dt = dateparser.parse(str(data_raw))
        return dt.date() if dt else None
    except:
        return None


# ==============================================================================
# CARGA RSS
# ==============================================================================
def carregar_rss_tempo_real(bert_analyzer):
    print("-> üì° Buscando RSS Tempo Real...")
    dados = []

    for setor, url in TOPICOS_RSS.items():
        print(f"   ...lendo RSS de {setor}")
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                titulo = entry.title
                link = entry.link
                fonte_rss = entry.source.get('title', '').lower() if 'source' in entry else ''

                if not validar_fonte_por_texto(fonte_rss) and not validar_fonte_por_texto(titulo):
                    continue

                try:
                    if hasattr(entry, 'published_parsed'):
                        data_pub = datetime.fromtimestamp(time.mktime(entry.published_parsed)).date()
                    else:
                        data_pub = datetime.now().date()
                except:
                    data_pub = datetime.now().date()

                if not validar_recencia(data_pub):
                    continue

                score = analisar_sentimento(titulo, bert_analyzer)
                dados.append((setor, titulo, score, data_pub, link))
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro RSS {setor}: {e}")
    return dados


# ==============================================================================
# CARGA DUCKDUCKGO
# ==============================================================================
def carregar_historico_completo(bert_analyzer, dias_atras=730):
    print(f"-> üï∞Ô∏è Iniciando Busca Hist√≥rica via DuckDuckGo (Sem 429!)...")
    dados = []
    ids_vistos = set()

    # Instancia o buscador
    ddgs = DDGS()

    for setor, lista_termos in TERMOS_HISTORICO.items():
        print(f"   üîé Setor: {setor}...")

        for termo in lista_termos:
            print(f"      Busca: '{termo}'")
            try:
                # max_results=25 por termo garante um bom volume hist√≥rico sem travar
                # region="br-pt" foca no Brasil
                resultados = ddgs.news(
                    keywords=termo,
                    region="br-pt",
                    safesearch="off",
                    max_results=30
                )

                if resultados:
                    count_termo = 0
                    for item in resultados:
                        # O DDG retorna dict: {'date':..., 'title':..., 'body':..., 'url':..., 'source':...}
                        titulo = item.get('title')
                        link = item.get('url')
                        source = item.get('source', '')
                        data_raw = item.get('date')

                        # Deduplica√ß√£o
                        if not titulo or titulo in ids_vistos: continue

                        # Valida√ß√£o de Fonte
                        if not validar_fonte_por_texto(link) and not validar_fonte_por_texto(source):
                            continue

                        # Tratamento de Data
                        data_pub = limpar_data_ddg(data_raw)

                        # Se n√£o conseguiu ler a data do DDG, assume uma aleat√≥ria recente (fallback)
                        # ou descarta. Vamos assumir aleat√≥ria nos √∫ltimos 6 meses para n√£o perder o dado.
                        if not data_pub:
                            dias_rand = random.randint(1, 180)
                            data_pub = (datetime.now() - timedelta(days=dias_rand)).date()

                        # Filtro de Rec√™ncia
                        if not validar_recencia(data_pub, dias_atras):
                            continue

                        # Sentimento
                        score = analisar_sentimento(titulo, bert_analyzer)

                        dados.append((setor, titulo, score, data_pub, link))
                        ids_vistos.add(titulo)
                        count_termo += 1

                    # print(f"         ‚úÖ {count_termo} not√≠cias coletadas.")

                # Pausa leve (DuckDuckGo √© r√°pido, 2s √© suficiente)
                time.sleep(2)

            except Exception as e:
                print(f"         ‚ö†Ô∏è Erro no termo '{termo}': {e}")
                time.sleep(5)  # Pausa um pouco maior se der erro

        # Pausa entre setores
        print(f"      ‚òï Mudando de setor... (5s)")
        time.sleep(5)

    return dados


# ==============================================================================
# ORQUESTRA√á√ÉO
# ==============================================================================
def executar_etl_noticias():
    print("\nüì∞ [ETL NLP] Iniciando Pipeline (DuckDuckGo Edition)...")

    try:
        bert_analyzer = create_analyzer(task="sentiment", lang="pt")
    except Exception as e:
        print(f"   ‚ùå Erro IA: {e}")
        return

    lista_final = []

    # 1. RSS
    news_rss = carregar_rss_tempo_real(bert_analyzer)
    lista_final.extend(news_rss)
    print(f"   üìä Not√≠cias RSS: {len(news_rss)}")

    # 2. Hist√≥rico (DuckDuckGo)
    # Aumentei para 730 dias (2 anos) j√° que o DDG aguenta
    news_hist = carregar_historico_completo(bert_analyzer, dias_atras=730)
    lista_final.extend(news_hist)
    print(f"   üï∞Ô∏è Not√≠cias Hist√≥ricas: {len(news_hist)}")

    if not lista_final:
        print("   ‚ö†Ô∏è Nenhuma not√≠cia coletada.")
        return

    try:
        conn = get_connection()
        if not conn: return

        if conn:
            cursor = conn.cursor()
            print("   üßπ Limpando tabela de not√≠cias...")

            # Limpa tudo antes de inserir
            cursor.execute("DELETE FROM T_BF_NOTICIAS")

            print(f"   üíæ Salvando {len(lista_final)} not√≠cias...")

            sql_insert = """
                         INSERT INTO T_BF_NOTICIAS (ds_setor, tx_titulo, vl_sentimento, dt_publicacao, tx_link)
                         VALUES (:1, :2, :3, :4, :5)
                         """

            batch_size = 500
            for i in range(0, len(lista_final), batch_size):
                batch = lista_final[i:i + batch_size]
                cursor.executemany(sql_insert, batch)

            conn.commit()
            print("   ‚úÖ Sucesso! Banco atualizado.")

    except Exception as e:
        if 'conn' in locals() and conn:
            conn.rollback()
        print(f"‚ùå Erro banco: {e}")
    finally:
        if 'conn' in locals() and conn:
            conn.close()